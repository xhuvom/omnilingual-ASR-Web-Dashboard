<!-- ?xml version='1.0' encoding='UTF-8'? -->
<link href="/github-markdown-css/github-css.css" rel="stylesheet"/>
<meta charset="utf-8" content="text/html"/>
<div class="gist">
<style class="formula-style">
        svg.gh-md-to-html-formula {
            fill: black;
        }
    </style>
<div class="gist-file"> <!-- This is the class that is responsible for the boxing! -->
<div class="gist-data">
<div class="js-gist-file-update-container js-task-list-container file-box">
<div class="file" id="user-content-article-ASR_finetuning_instructions">
<div class="Box-body readme blob js-code-block-container p-5 p-xl-6" id="user-content-file-docker-image-pull-md-readme" style="margin-left: 40px; margin-right: 40px; margin-top: 20px; margin-bottom: 20px">
<article class="markdown-body entry-content container-lg" itemprop="text">
<div class="markdown-heading"><h1 class="heading-element">ASR Finetuning Guide: Bangla &amp; Banglish (Code-Switching)</h1><a aria-label="Permalink: ASR Finetuning Guide: Bangla &amp; Banglish (Code-Switching)" class="anchor" href="#user-content-asr-finetuning-guide-bangla--banglish-code-switching" id="user-content-asr-finetuning-guide-bangla--banglish-code-switching" name="user-content-asr-finetuning-guide-bangla--banglish-code-switching"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>This document provides a comprehensive analysis and step-by-step guide for fine-tuning ASR models for your specific use case (Bangla/English mixed "Banglish").</p>
<div class="markdown-heading"><h2 class="heading-element">1. Model Selection &amp; Verification</h2><a aria-label="Permalink: 1. Model Selection &amp; Verification" class="anchor" href="#user-content-1-model-selection--verification" id="user-content-1-model-selection--verification" name="user-content-1-model-selection--verification"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<div class="markdown-heading"><h3 class="heading-element"><strong>Hypothesis Verification</strong></h3><a aria-label="Permalink: Hypothesis Verification" class="anchor" href="#user-content-hypothesis-verification" id="user-content-hypothesis-verification" name="user-content-hypothesis-verification"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<blockquote>
<p><strong>Your Hypothesis:</strong> "Finetuning can make CTC competitive... CTC can learn it if training data shows proper script separation."</p>
</blockquote>
<p><strong>Verdict: CORRECT &amp; RECOMMENDED</strong></p>
<ul>
<li>
<p><strong>Why CTC (Connectionist Temporal Classification) is better for you:</strong></p>
<ol>
<li>
<strong>Data Efficiency:</strong> CTC models (like Wav2Vec2/MMS-1B) are strictly acoustic models. They learn to map sound to characters directly. They are less prone to "hallucination" than LLM-based decoders when training data is scarce (&lt; 100 examples).</li>
<li>
<strong>Code-Switching:</strong> CTC handles code-switching naturally as long as the vocabulary contains both scripts (Bengali and Latin). It doesn't rely on a complex language model that might bias it towards "pure" Bangla.</li>
<li>
<strong>Hardware Friendly:</strong> You can fine-tune a 1B CTC model on your GPUs (see below) much more easily than the "LLM" variants.</li>
</ol>
</li>
<li>
<p><strong>Why NOT the "LLM" model (Wav2Vec2 + LLaMA) for now:</strong></p>
<ul>
<li>
<strong>Architecture:</strong> The <code>omniASR_LLM_1B</code> appears to be a composite of a Wav2Vec2 encoder + a LLaMA-style decoder (likely ~2B-3B params total).</li>
<li>
<strong>Training Difficulty:</strong> Training this requires backpropagation through the decoder. With only 20-100 examples, the decoder will likely <strong>overfit</strong> or become unstable (forgetting the pre-trained knowledge) or hallucinate.</li>
<li>
<strong>VRAM Usage:</strong> It is significantly heavier and might not fit on the 8GB GPU even with LoRA without aggressive optimization (4-bit quantization).</li>
</ul>
</li>
</ul>
<p><strong>Recommendation:</strong> Stick to <strong><code>omniASR-CTC-1B</code></strong> (or its Hugging Face equivalent <code>facebook/mms-1b-all</code>) for your fine-tuning experiments.</p>
<hr/>
<div class="markdown-heading"><h2 class="heading-element">2. Hardware Capability &amp; GPU Selection</h2><a aria-label="Permalink: 2. Hardware Capability &amp; GPU Selection" class="anchor" href="#user-content-2-hardware-capability--gpu-selection" id="user-content-2-hardware-capability--gpu-selection" name="user-content-2-hardware-capability--gpu-selection"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>You have:</p>
<ul>
<li>
<strong>PC 1:</strong> RTX 2070? (8GB VRAM typically).</li>
<li>
<strong>PC 2:</strong> RTX 2060 12GB.</li>
</ul>
<div class="markdown-heading"><h3 class="heading-element"><strong>Finetuning <code>CTC-1B</code> (1 Billion Parameters)</strong></h3><a aria-label="Permalink: Finetuning CTC-1B (1 Billion Parameters)" class="anchor" href="#user-content-finetuning-ctc-1b-1-billion-parameters" id="user-content-finetuning-ctc-1b-1-billion-parameters" name="user-content-finetuning-ctc-1b-1-billion-parameters"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<table>
<thead>
<tr>
<th align="left">Method</th>
<th align="left">VRAM Estimate</th>
<th align="left">PC 1 (8GB)</th>
<th align="left">PC 2 (12GB)</th>
<th align="left">Speed</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><strong>Full Finetuning</strong></td>
<td align="left">~14-16 GB</td>
<td align="left">❌ No</td>
<td align="left">❌ No (Too tight)</td>
<td align="left">Fast</td>
</tr>
<tr>
<td align="left"><strong>LoRA (PEFT)</strong></td>
<td align="left">~6-8 GB</td>
<td align="left">✅ <strong>Yes</strong>
</td>
<td align="left">✅ <strong>Yes</strong>
</td>
<td align="left">Fast</td>
</tr>
<tr>
<td align="left"><strong>Frozen Encoder</strong></td>
<td align="left">~4-6 GB</td>
<td align="left">✅ <strong>Yes</strong>
</td>
<td align="left">✅ <strong>Yes</strong>
</td>
<td align="left">Fastest</td>
</tr>
</tbody>
</table>
<ul>
<li>
<strong>Conclusion:</strong> You <strong>CAN</strong> fine-tune the CTC 1B model on <strong>both</strong> machines if you use <strong>LoRA (Low-Rank Adaptation)</strong> or <strong>freeze the feature encoder</strong>.</li>
<li>
<strong>Best Practice:</strong> Use the <strong>12GB GPU (RTX 2060)</strong>. The extra VRAM allows for larger batch sizes (more stable gradients) and prevents OOM (Out of Memory) crashes.</li>
</ul>
<hr/>
<div class="markdown-heading"><h2 class="heading-element">3. Training / Finetuning Instructions</h2><a aria-label="Permalink: 3. Training / Finetuning Instructions" class="anchor" href="#user-content-3-training--finetuning-instructions" id="user-content-3-training--finetuning-instructions" name="user-content-3-training--finetuning-instructions"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Since the <code>omnilingual-asr</code> repo you cloned (fairseq2 based) lacks a ready-to-use Training script (it is primarily configured for inference), the <strong>industry standard best practice</strong> is to use <strong>Hugging Face Transformers</strong>.</p>
<p>The <code>omniASR-CTC-1B</code> is essentially Meta's <strong>MMS-1B</strong> model (<code>facebook/mms-1b-all</code>).</p>
<div class="markdown-heading"><h3 class="heading-element"><strong>Preparation</strong></h3><a aria-label="Permalink: Preparation" class="anchor" href="#user-content-preparation" id="user-content-preparation" name="user-content-preparation"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>
<p><strong>Install Libraries</strong> (in a new or existing environment):</p>
<div class="highlight highlight-source-shell"><pre>pip install transformers datasets accelerate peft bitsandbytes librosa torch evaluate jiwer</pre></div>
</li>
<li>
<p><strong>Data Preparation (Crucial for Banglish)</strong>
Create a dataset CSV/JSON with your 20-100 examples.</p>
<p><em>Structure:</em> <code>audio_path</code>, <code>sentence</code></p>
<p><strong>Text Normalization Rules (Best Practices):</strong></p>
<ul>
<li>
<strong>Bangla:</strong> Keep standard spelling.</li>
<li>
<strong>English/Banglish:</strong> Decide on a convention.
<ul>
<li>Option A (Transliterated): "ami vat khai" (All Latin)</li>
<li>Option B (Mixed Script): "আমি ভাত eat করি" (Mixed)</li>
</ul>
</li>
<li>
<strong>The model needs to learn your specific convention.</strong> Ensure your training data matches how you want the output to look.</li>
<li>
<strong>Vocab:</strong> The MMS model usually supports both scripts.</li>
</ul>
</li>
</ol>
<div class="markdown-heading"><h3 class="heading-element"><strong>Training Script (Template)</strong></h3><a aria-label="Permalink: Training Script (Template)" class="anchor" href="#user-content-training-script-template" id="user-content-training-script-template" name="user-content-training-script-template"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Create a file named <code>finetune_mms.py</code> and accept the following template:</p>
<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">from</span> <span class="pl-s1">transformers</span> <span class="pl-k">import</span> <span class="pl-v">Wav2Vec2ForCTC</span>, <span class="pl-v">AutoProcessor</span>, <span class="pl-v">TrainingArguments</span>, <span class="pl-v">Trainer</span>
<span class="pl-k">from</span> <span class="pl-s1">datasets</span> <span class="pl-k">import</span> <span class="pl-s1">load_dataset</span>, <span class="pl-v">Audio</span>
<span class="pl-k">from</span> <span class="pl-s1">peft</span> <span class="pl-k">import</span> <span class="pl-v">LoraConfig</span>, <span class="pl-s1">get_peft_model</span>
<span class="pl-k">import</span> <span class="pl-s1">numpy</span> <span class="pl-k">as</span> <span class="pl-s1">np</span>

<span class="pl-c"># 1. Config</span>
<span class="pl-c1">MODEL_ID</span> <span class="pl-c1">=</span> <span class="pl-s">"facebook/mms-1b-all"</span> <span class="pl-c"># Equivalent to omniASR-CTC-1B</span>
<span class="pl-c1">OUTPUT_DIR</span> <span class="pl-c1">=</span> <span class="pl-s">"mms-1b-banglish-finetuned"</span>
<span class="pl-c1">USE_LORA</span> <span class="pl-c1">=</span> <span class="pl-c1">True</span>

<span class="pl-c"># 2. Load Processor (Tokenizer + Feature Extractor)</span>
<span class="pl-s1">processor</span> <span class="pl-c1">=</span> <span class="pl-v">AutoProcessor</span>.<span class="pl-c1">from_pretrained</span>(<span class="pl-c1">MODEL_ID</span>)
<span class="pl-c"># Ensure the tokenizer has all characters you need. </span>
<span class="pl-c"># If validation fails on new characters, you might need to add them to the tokenizer adapter:</span>
<span class="pl-c"># tokenizer = processor.tokenizer</span>

<span class="pl-c"># 3. Load Dataset</span>
<span class="pl-c"># Assuming you have a CSV with 'path' and 'sentence' columns</span>
<span class="pl-c"># dataset = load_dataset("csv", data_files="my_banglish_data.csv")</span>
<span class="pl-c"># For demo, using dummy data structure. REPLACE above line with your CSV load.</span>
<span class="pl-c"># dataset = dataset["train"].train_test_split(test_size=0.1) # Split for validation</span>

<span class="pl-c"># 4. Preprocessing</span>
<span class="pl-k">def</span> <span class="pl-en">prepare_dataset</span>(<span class="pl-s1">batch</span>):
    <span class="pl-s1">audio</span> <span class="pl-c1">=</span> <span class="pl-s1">batch</span>[<span class="pl-s">"audio"</span>]
    <span class="pl-s1">batch</span>[<span class="pl-s">"input_values"</span>] <span class="pl-c1">=</span> <span class="pl-en">processor</span>(<span class="pl-s1">audio</span>[<span class="pl-s">"array"</span>], <span class="pl-s1">sampling_rate</span><span class="pl-c1">=</span><span class="pl-c1">16000</span>).<span class="pl-c1">input_values</span>[<span class="pl-c1">0</span>]
    <span class="pl-k">with</span> <span class="pl-s1">processor</span>.<span class="pl-c1">as_target_processor</span>():
        <span class="pl-s1">batch</span>[<span class="pl-s">"labels"</span>] <span class="pl-c1">=</span> <span class="pl-en">processor</span>(<span class="pl-s1">batch</span>[<span class="pl-s">"sentence"</span>]).<span class="pl-c1">input_ids</span>
    <span class="pl-k">return</span> <span class="pl-s1">batch</span>

<span class="pl-c"># 5. Load Model</span>
<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-v">Wav2Vec2ForCTC</span>.<span class="pl-c1">from_pretrained</span>(
    <span class="pl-c1">MODEL_ID</span>, 
    <span class="pl-s1">attention_dropout</span><span class="pl-c1">=</span><span class="pl-c1">0.1</span>,
    <span class="pl-s1">hidden_dropout</span><span class="pl-c1">=</span><span class="pl-c1">0.1</span>,
    <span class="pl-s1">feat_proj_dropout</span><span class="pl-c1">=</span><span class="pl-c1">0.1</span>,
    <span class="pl-s1">target_lang</span><span class="pl-c1">=</span><span class="pl-s">"ben"</span>, <span class="pl-c"># Initialize with Bengali adapter settings</span>
    <span class="pl-s1">ignore_mismatched_sizes</span><span class="pl-c1">=</span><span class="pl-c1">True</span>, <span class="pl-c"># In case we resize vocab</span>
    <span class="pl-s1">torch_dtype</span><span class="pl-c1">=</span><span class="pl-s1">torch</span>.<span class="pl-c1">float16</span> <span class="pl-c"># Half precision for GPU memory</span>
)
<span class="pl-s1">model</span>.<span class="pl-c1">init_adapter_layers</span>() <span class="pl-c"># Activate adapters if MMS uses them</span>
<span class="pl-s1">model</span>.<span class="pl-c1">freeze_base_model</span>() <span class="pl-c"># Optional: Freeze base to save memory, strictly finetune adapter/head</span>

<span class="pl-k">if</span> <span class="pl-c1">USE_LORA</span>:
    <span class="pl-s1">peft_config</span> <span class="pl-c1">=</span> <span class="pl-en">LoraConfig</span>(
        <span class="pl-s1">inference_mode</span><span class="pl-c1">=</span><span class="pl-c1">False</span>, <span class="pl-s1">r</span><span class="pl-c1">=</span><span class="pl-c1">16</span>, <span class="pl-s1">lora_alpha</span><span class="pl-c1">=</span><span class="pl-c1">32</span>, <span class="pl-s1">lora_dropout</span><span class="pl-c1">=</span><span class="pl-c1">0.1</span>, <span class="pl-s1">bias</span><span class="pl-c1">=</span><span class="pl-s">"none"</span>,
        <span class="pl-s1">target_modules</span><span class="pl-c1">=</span>[<span class="pl-s">"q_proj"</span>, <span class="pl-s">"v_proj"</span>] <span class="pl-c"># Target attention layers</span>
    )
    <span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-en">get_peft_model</span>(<span class="pl-s1">model</span>, <span class="pl-s1">peft_config</span>)
    <span class="pl-s1">model</span>.<span class="pl-c1">print_trainable_parameters</span>()

<span class="pl-c"># 6. Training Arguments</span>
<span class="pl-s1">training_args</span> <span class="pl-c1">=</span> <span class="pl-en">TrainingArguments</span>(
    <span class="pl-s1">output_dir</span><span class="pl-c1">=</span><span class="pl-c1">OUTPUT_DIR</span>,
    <span class="pl-s1">group_by_length</span><span class="pl-c1">=</span><span class="pl-c1">True</span>,
    <span class="pl-s1">per_device_train_batch_size</span><span class="pl-c1">=</span><span class="pl-c1">2</span>, <span class="pl-c"># Keep small for 8GB/12GB GPU</span>
    <span class="pl-s1">gradient_accumulation_steps</span><span class="pl-c1">=</span><span class="pl-c1">4</span>, <span class="pl-c"># Effective batch size = 8</span>
    <span class="pl-s1">evaluation_strategy</span><span class="pl-c1">=</span><span class="pl-s">"steps"</span>,
    <span class="pl-s1">num_train_epochs</span><span class="pl-c1">=</span><span class="pl-c1">10</span>, <span class="pl-c"># 10-30 epochs for small data</span>
    <span class="pl-s1">fp16</span><span class="pl-c1">=</span><span class="pl-c1">True</span>, <span class="pl-c"># Essential for VRAM saving</span>
    <span class="pl-s1">save_steps</span><span class="pl-c1">=</span><span class="pl-c1">50</span>,
    <span class="pl-s1">eval_steps</span><span class="pl-c1">=</span><span class="pl-c1">50</span>,
    <span class="pl-s1">logging_steps</span><span class="pl-c1">=</span><span class="pl-c1">10</span>,
    <span class="pl-s1">learning_rate</span><span class="pl-c1">=</span><span class="pl-c1">3e-4</span>, <span class="pl-c"># Slightly higher for LoRA</span>
    <span class="pl-s1">warmup_steps</span><span class="pl-c1">=</span><span class="pl-c1">50</span>,
    <span class="pl-s1">save_total_limit</span><span class="pl-c1">=</span><span class="pl-c1">2</span>,
    <span class="pl-s1">dataloader_num_workers</span><span class="pl-c1">=</span><span class="pl-c1">2</span>,
)

<span class="pl-c"># 7. Data Collator (Pads audio and labels dynamically)</span>
<span class="pl-k">from</span> <span class="pl-s1">transformers</span> <span class="pl-k">import</span> <span class="pl-v">DataCollatorCTCWithPadding</span>
<span class="pl-s1">data_collator</span> <span class="pl-c1">=</span> <span class="pl-en">DataCollatorCTCWithPadding</span>(<span class="pl-s1">processor</span><span class="pl-c1">=</span><span class="pl-s1">processor</span>, <span class="pl-s1">padding</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)

<span class="pl-c"># 8. Train</span>
<span class="pl-c"># trainer = Trainer(</span>
<span class="pl-c">#     model=model,</span>
<span class="pl-c">#     data_collator=data_collator,</span>
<span class="pl-c">#     args=training_args,</span>
<span class="pl-c">#     train_dataset=dataset["train"],</span>
<span class="pl-c">#     eval_dataset=dataset["test"],</span>
<span class="pl-c">#     tokenizer=processor.feature_extractor,</span>
<span class="pl-c"># )</span>

<span class="pl-c"># trainer.train()</span></pre></div>
<div class="markdown-heading"><h2 class="heading-element">4. Best Practices for Your Use Case</h2><a aria-label="Permalink: 4. Best Practices for Your Use Case" class="anchor" href="#user-content-4-best-practices-for-your-use-case" id="user-content-4-best-practices-for-your-use-case" name="user-content-4-best-practices-for-your-use-case"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>
<p><strong>Mixed Script Strategy:</strong></p>
<ul>
<li>Input: <code>Audio("আমি ভাত খাই")</code> -&gt; Label: <code>"ami vat khai"</code> (if you want transliteration).</li>
<li>Input: <code>Audio("I eat rice")</code> -&gt; Label: <code>"I eat rice"</code> (if you want English).</li>
<li>
<strong>Crucial:</strong> Ensure your text labels strictly match what you said. If you used an English word in the audio but wrote the Bangla translation in the label, the model will get confused.</li>
</ul>
</li>
<li>
<p><strong>Audio Quality:</strong></p>
<ul>
<li>For 20-100 examples, clean audio is better.</li>
<li>However, if you want robustness, add some background noise (office noise, street noise) to half of your examples (Data Augmentation).</li>
</ul>
</li>
<li>
<p><strong>Evaluation:</strong></p>
<ul>
<li>Don't trust the loss alone. Look at the generated transcriptions on your test set.</li>
<li>Use <strong>CER (Character Error Rate)</strong> rather than WER (Word Error Rate) for Bangla/Banglish, as spelling variations (transliteration) can inflate WER.</li>
</ul>
</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">5. Handling Numerics, Alphanumerics &amp; Pauses (Advanced)</h2><a aria-label="Permalink: 5. Handling Numerics, Alphanumerics &amp; Pauses (Advanced)" class="anchor" href="#user-content-5-handling-numerics-alphanumerics--pauses-advanced" id="user-content-5-handling-numerics-alphanumerics--pauses-advanced" name="user-content-5-handling-numerics-alphanumerics--pauses-advanced"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>For your requirement of <strong>English/Bangla digits (0-9)</strong>, <strong>Long IDs</strong>, <strong>Passport numbers</strong>, and <strong>Pauses</strong>:</p>
<div class="markdown-heading"><h3 class="heading-element"><strong>A. Tokenizer Configuration (Crucial)</strong></h3><a aria-label="Permalink: A. Tokenizer Configuration (Crucial)" class="anchor" href="#user-content-a-tokenizer-configuration-crucial" id="user-content-a-tokenizer-configuration-crucial" name="user-content-a-tokenizer-configuration-crucial"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Standard ASR pipelines often normalize numbers to text (e.g., "10" -&gt; "ten"). <strong>You must disable this behavior</strong> and ensuring your tokenizer recognizes digits.</p>
<ol>
<li>
<strong>Check Vocab:</strong> The base MMS model might <em>not</em> have <code>0</code>, <code>1</code>...<code>9</code> in its vocabulary.</li>
<li>
<strong>Add Tokens:</strong> You must check and add them if missing.</li>
</ol>
<p><strong>Updated Training Snippet (Add to <code>finetune_mms.py</code> before loading model):</strong></p>
<div class="highlight highlight-source-python"><pre><span class="pl-c"># ... load processor ...</span>
<span class="pl-s1">processor</span> <span class="pl-c1">=</span> <span class="pl-v">AutoProcessor</span>.<span class="pl-c1">from_pretrained</span>(<span class="pl-c1">MODEL_ID</span>)

<span class="pl-c"># 1. Add Digits and Alphanumeric chars if missing</span>
<span class="pl-s1">new_tokens</span> <span class="pl-c1">=</span> [<span class="pl-s">"0"</span>, <span class="pl-s">"1"</span>, <span class="pl-s">"2"</span>, <span class="pl-s">"3"</span>, <span class="pl-s">"4"</span>, <span class="pl-s">"5"</span>, <span class="pl-s">"6"</span>, <span class="pl-s">"7"</span>, <span class="pl-s">"8"</span>, <span class="pl-s">"9"</span>,
              <span class="pl-s">"A"</span>, <span class="pl-s">"B"</span>, <span class="pl-s">"C"</span>, <span class="pl-s">"D"</span>, <span class="pl-s">"E"</span>, <span class="pl-s">"F"</span>, <span class="pl-s">"G"</span>, <span class="pl-s">"H"</span>, <span class="pl-s">"I"</span>, <span class="pl-s">"J"</span>, <span class="pl-s">"K"</span>, <span class="pl-s">"L"</span>, <span class="pl-s">"M"</span>,
              <span class="pl-s">"N"</span>, <span class="pl-s">"O"</span>, <span class="pl-s">"P"</span>, <span class="pl-s">"Q"</span>, <span class="pl-s">"R"</span>, <span class="pl-s">"S"</span>, <span class="pl-s">"T"</span>, <span class="pl-s">"U"</span>, <span class="pl-s">"V"</span>, <span class="pl-s">"W"</span>, <span class="pl-s">"X"</span>, <span class="pl-s">"Y"</span>, <span class="pl-s">"Z"</span>]

<span class="pl-c"># Check if tokenizer has them, if not add them</span>
<span class="pl-s1">missing_tokens</span> <span class="pl-c1">=</span> [<span class="pl-s1">t</span> <span class="pl-k">for</span> <span class="pl-s1">t</span> <span class="pl-c1">in</span> <span class="pl-s1">new_tokens</span> <span class="pl-k">if</span> <span class="pl-s1">t</span> <span class="pl-c1"><span class="pl-c1">not</span> <span class="pl-c1">in</span></span> <span class="pl-s1">processor</span>.<span class="pl-c1">tokenizer</span>.<span class="pl-c1">get_vocab</span>()]
<span class="pl-k">if</span> <span class="pl-s1">missing_tokens</span>:
    <span class="pl-en">print</span>(<span class="pl-s">f"Adding missing tokens: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">missing_tokens</span><span class="pl-kos">}</span></span>"</span>)
    <span class="pl-s1">processor</span>.<span class="pl-c1">tokenizer</span>.<span class="pl-c1">add_tokens</span>(<span class="pl-s1">missing_tokens</span>)
    <span class="pl-c"># IMPORTANT: We will need to resize the model embeddings later</span></pre></div>
<p><em>And after loading the model:</em></p>
<div class="highlight highlight-source-python"><pre><span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-v">Wav2Vec2ForCTC</span>.<span class="pl-c1">from_pretrained</span>(...)
<span class="pl-k">if</span> <span class="pl-s1">missing_tokens</span>:
    <span class="pl-s1">model</span>.<span class="pl-c1">resize_token_embeddings</span>(<span class="pl-en">len</span>(<span class="pl-s1">processor</span>.<span class="pl-c1">tokenizer</span>))</pre></div>
<div class="markdown-heading"><h3 class="heading-element"><strong>B. Labeling Strategy for Pauses &amp; Grouping</strong></h3><a aria-label="Permalink: B. Labeling Strategy for Pauses &amp; Grouping" class="anchor" href="#user-content-b-labeling-strategy-for-pauses--grouping" id="user-content-b-labeling-strategy-for-pauses--grouping" name="user-content-b-labeling-strategy-for-pauses--grouping"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>CTC models are great at learning silence/pauses if you label them with <strong>spaces</strong> or <strong>commas</strong>.</p>
<ul>
<li>
<p><strong>Scenario 1: Phone Numbers with pauses</strong></p>
<ul>
<li>
<em>Speech:</em> "Zero one seven... [pause] ... two four six..."</li>
<li>
<em>Incorrect Label:</em> <code>017246</code> (Model learns to rush the output)</li>
<li>
<em>Correct Label:</em> <code>017 246</code> or <code>017, 246</code>
</li>
<li>
<strong>Recommendation:</strong> Use <strong>spaces</strong> for human-like pauses. The model will learn to predict a space token when the speaker pauses.</li>
</ul>
</li>
<li>
<p><strong>Scenario 2: Mixed Script Alphanumeric (Passport)</strong></p>
<ul>
<li>
<em>Speech:</em> "Passport number... [pause] ... A zero two..."</li>
<li>
<em>Label:</em> <code>Passport number A02</code> (If spoken continuously)</li>
<li>
<em>Label:</em> <code>Passport number A 02</code> (If explicitly paused)</li>
</ul>
</li>
</ul>
<div class="markdown-heading"><h3 class="heading-element"><strong>C. 500 Examples Optimization</strong></h3><a aria-label="Permalink: C. 500 Examples Optimization" class="anchor" href="#user-content-c-500-examples-optimization" id="user-content-c-500-examples-optimization" name="user-content-c-500-examples-optimization"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>With 500 examples, you are entering the "safe zone" for more robust finetuning.</p>
<ol>
<li>
<p><strong>Unfreeze More Layers:</strong> Instead of just the adapter/LoRA, you can try unfreezing the last 1-2 transformer layers of the encoder <em>after</em> initial convergence (though LoRA is still safer on 8GB VRAM).</p>
</li>
<li>
<p><strong>Learning Rate:</strong> You can increase the learning rate slightly (e.g., <code>1e-4</code> to <code>3e-4</code>) since you have more signal.</p>
</li>
<li>
<p><strong>Epochs:</strong> Increase to 20-40 epochs. With 500 items, one epoch is fast.</p>
</li>
<li>
<p><strong>Local Training Workflow:</strong></p>
<ul>
<li>SSH into the <strong>12GB GPU</strong> machine.</li>
<li>Run the script using <code>python finetune_mms.py</code>.</li>
<li>Monitor VRAM with <code>watch -n 1 nvidia-smi</code>.</li>
<li>If OOM (Out of Memory):
<ul>
<li>Reduce <code>per_device_train_batch_size</code> to 1.</li>
<li>Enable <code>gradient_checkpointing=True</code> in <code>TrainingArguments</code>.</li>
</ul>
</li>
</ul>
</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">6. Cloud GPU Strategy (24GB+ VRAM)</h2><a aria-label="Permalink: 6. Cloud GPU Strategy (24GB+ VRAM)" class="anchor" href="#user-content-6-cloud-gpu-strategy-24gb-vram" id="user-content-6-cloud-gpu-strategy-24gb-vram" name="user-content-6-cloud-gpu-strategy-24gb-vram"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>If you rent a <strong>A10G, RTX 3090/4090, or A100 (24GB-40GB)</strong>, you can skip the "low memory" optimizations and prioritize <strong>Speed</strong> and <strong>Quality</strong>.</p>
<div class="markdown-heading"><h3 class="heading-element"><strong>A. Changes to Training Config</strong></h3><a aria-label="Permalink: A. Changes to Training Config" class="anchor" href="#user-content-a-changes-to-training-config" id="user-content-a-changes-to-training-config" name="user-content-a-changes-to-training-config"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>You no longer need 4-bit loading or aggressive gradient checkpointing (which slows down training).</p>
<ol>
<li>
<strong>Disable Quantization:</strong> Load model in full <code>float16</code> or <code>bfloat16</code> (if Ampere+).
<div class="highlight highlight-source-python"><pre><span class="pl-c"># Remove load_in_4bit=True</span>
<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-v">Wav2Vec2ForCTC</span>.<span class="pl-c1">from_pretrained</span>(..., <span class="pl-s1">torch_dtype</span><span class="pl-c1">=</span><span class="pl-s1">torch</span>.<span class="pl-c1">float16</span>)</pre></div>
</li>
<li>
<strong>Disable Gradient Checkpointing:</strong>
<div class="highlight highlight-source-python"><pre><span class="pl-s1">training_args</span> <span class="pl-c1">=</span> <span class="pl-en">TrainingArguments</span>(..., <span class="pl-s1">gradient_checkpointing</span><span class="pl-c1">=</span><span class="pl-c1">False</span>) <span class="pl-c"># 20-30% faster</span></pre></div>
</li>
<li>
<strong>Increase Batch Size:</strong>
<ul>
<li>Set <code>per_device_train_batch_size=8</code> or <code>16</code>.</li>
<li>This provides a much more stable gradient estimate than batch size 1 or 2.</li>
</ul>
</li>
</ol>
<div class="markdown-heading"><h3 class="heading-element"><strong>B. Full Finetuning vs. LoRA</strong></h3><a aria-label="Permalink: B. Full Finetuning vs. LoRA" class="anchor" href="#user-content-b-full-finetuning-vs-lora" id="user-content-b-full-finetuning-vs-lora" name="user-content-b-full-finetuning-vs-lora"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>With 24GB, you <em>can</em> technically fully finetune the 1B model (unfreeze all layers).</p>
<ul>
<li>
<strong>Recommendation:</strong> <strong>Stick to LoRA for &lt; 1000 examples.</strong>
</li>
<li>
<strong>Why?</strong> A 1B parameter model will memorize 500 examples instantly (overfitting). LoRA acts as a regularizer.</li>
<li>
<strong>When to Full Finetune:</strong> Once you have <strong>10+ hours</strong> of data (approx. 2000+ examples).</li>
</ul>
<div class="markdown-heading"><h3 class="heading-element"><strong>C. Optimal Data &amp; Time Estimates</strong></h3><a aria-label="Permalink: C. Optimal Data &amp; Time Estimates" class="anchor" href="#user-content-c-optimal-data--time-estimates" id="user-content-c-optimal-data--time-estimates" name="user-content-c-optimal-data--time-estimates"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>1. How much data is "Optimum"?</strong>
For "Banglish" (a dialet/mix not in base training):</p>
<ul>
<li>
<strong>Minimum (Functionality):</strong> 100 examples (~15 mins). <em>Result: Works for specific phrases.</em>
</li>
<li>
<strong>Robust (Variance):</strong> 500-800 examples (~1-2 hours). <em>Result: Generalizes to new numbers/sentences.</em>
</li>
<li>
<strong>Production (High Accuracy):</strong> 2000+ examples (~5+ hours). <em>Result: Professional grade.</em>
</li>
</ul>
<p><strong>2. Training Time Estimate (on 24GB GPU)</strong>
Assuming 500 examples (avg 5s each = ~42 mins of audio):</p>
<table>
<thead>
<tr>
<th align="left">Task</th>
<th align="left">Configuration</th>
<th align="left">Est. Time (10 Epochs)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><strong>LoRA (Optimized)</strong></td>
<td align="left">Batch 16, fp16</td>
<td align="left"><strong>~5 - 8 Minutes</strong></td>
</tr>
<tr>
<td align="left"><strong>Full Finetune</strong></td>
<td align="left">Batch 8, fp16</td>
<td align="left"><strong>~15 - 20 Minutes</strong></td>
</tr>
</tbody>
</table>
<p><strong>Conclusion:</strong> Training is extremely fast on this scale. You can iterate quickly.</p>
<ul>
<li>
<strong>Workflow:</strong> Train 500 examples -&gt; Test -&gt; Add 100 failed examples -&gt; Retrain (Active Learning).</li>
</ul>
<p>This approach gives you the best chance of success with limited data and hardware.</p>
</article>
</div>
</div>
</div>
</div>
</div>
</div>
